# Test Evaluation Configuration
# Minimal config for testing the evaluation pipeline

evaluation:
  name: "Test Evaluation"
  output_dir: "output/evaluation_test"
  parallel_jobs: 2
  overwrite: true
  
  kpis:
    - ate_rmse
    - runtime_ms

datasets:
  simulated:
    generate_if_missing: true
    types:
      - name: "circle"
        config:
          trajectory:
            type: "circle"
            radius: 5.0
            duration: 10.0  # Short duration for testing
            rate: 100.0
          noise:
            add_noise: true
            imu_noise_level: 0.1
            camera_noise_level: 1.0
          landmarks:
            num_landmarks: 50  # Fewer landmarks for speed

  # External datasets removed - only simulated datasets supported

estimators:
  ekf:
    enabled: true
    config:
      process_noise:
        position: 0.01
        orientation: 0.001
        velocity: 0.1
        bias_accel: 0.001
        bias_gyro: 0.0001
      measurement_noise:
        camera: 1.0
      max_iterations: 10
      
  swba:
    enabled: true
    config:
      window_size: 5
      solver:
        type: "gauss_newton"
        max_iterations: 5

evaluation_settings:
  metrics:
    compute_ate: true
    compute_rpe: false  # Skip for speed
    compute_consistency: false
    compute_timing: true
    
  visualization:
    generate_plots: true
    generate_videos: false

dashboard:
  title: "Test Evaluation Dashboard"
  layout: "grid"
  
  sections:
    - name: "Overview"
      type: "summary_table"
      metrics: ["ate_rmse", "runtime_ms"]
      
    - name: "Performance Matrix"
      type: "heatmap"
      x_axis: "datasets"
      y_axis: "estimators"
      metric: "ate_rmse"

logging:
  level: "INFO"
  console: true
  
resources:
  max_memory_gb: 4
  timeout_minutes: 5